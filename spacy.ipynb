{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530155b2",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0072ce;\">spaCy</span> for Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220b5b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7e9debf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 29.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.3)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: setuptools in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (58.1.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\code\\nltk_spacy\\env\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'd:\\Code\\NLTK_Spacy\\env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c435e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7b82f-2ce0-49ad-b9df-46aa728c2285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc6584fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.lang.en.English'>\n"
     ]
    }
   ],
   "source": [
    "print(type(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b902d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff15d57f",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4be3d94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I don't like implementing neural nets in Tensorflow.\"\n",
    "doc = nlp(sentence)\n",
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fddf88",
   "metadata": {},
   "source": [
    "When we pass our senetence to our language instance nlp, it returns a **Doc** container object. A **Doc** is a sequence of **Token** objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b9881f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'do', \"n't\", 'like', 'implementing', 'neural', 'nets', 'in', 'Tensorflow', '.']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c16ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 0), ('do', 1), (\"n't\", 2), ('like', 3), ('implementing', 4), ('neural', 5), ('nets', 6), ('in', 7), ('Tensorflow', 8), ('.', 9)]\n"
     ]
    }
   ],
   "source": [
    "print([(token.text, token.i) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e519c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow\n",
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "print(doc[-2])\n",
    "print(type(doc[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf1516fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implementing neural nets\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "print(doc[4:7])\n",
    "print(type(doc[4:7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73dee5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't like implementing neural nets in Tensorflow.\n"
     ]
    }
   ],
   "source": [
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a88fcf5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I've been using PyTorch a few months now and I've never felt better.,\n",
       " I have more energy.,\n",
       " My skin is clearer.,\n",
       " My eye sight has improved.]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "karpathy_tweet = \"\"\"I've been using PyTorch a few months now and I've never felt better.\\\n",
    "I have more energy. My skin is clearer. My eye sight has improved.\"\"\"\n",
    "\n",
    "doc = nlp(karpathy_tweet)\n",
    "\n",
    "[sentence for sentence in doc.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d631c823",
   "metadata": {},
   "source": [
    "You can also use **NLTK** for tokenization if you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "919b40f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'do', \"n't\", 'like', 'implementing', 'neural', 'nets', 'in', 'Tensorflow', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"I don't like implementing neural nets in Tensorflow.\"\n",
    "\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b6ed7",
   "metadata": {},
   "source": [
    "## Case-Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "078ccee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"He told Dr. Lovato that he was done with the tests and would post the results shortly.\"\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11cceae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'told', 'dr.', 'lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'tests', 'and', 'would', 'post', 'the', 'results', 'shortly', '.']\n"
     ]
    }
   ],
   "source": [
    "print([token.lower_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bc38bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[He, 'told', 'dr.', 'lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'tests', 'and', 'would', 'post', 'the', 'results', 'shortly', '.']\n"
     ]
    }
   ],
   "source": [
    "print([token.lower_ if not token.is_sent_start else token for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8222b3",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58f0101c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'everyone', 'to', 'all', 'very', 'â€™s', 'yourselves', 'i', 'during', 'â€˜ll', 'yourself', 'whenever', 'here', 'do', 'she', 'each', 'while', 'only', 'top', 'same', 'as', 'about', 'through', 'formerly', 'via', 'hereafter', 'now', 'seem', 'yet', 'â€˜re', 'eight', 'may', 'him', 'must', 'across', 'give', 'it', 'above', 'a', 'our', 'made', 'cannot', 'within', \"'m\", 'namely', 'put', 'throughout', 'these', 'how', 'my', 'have', 'though', 'sixty', 'but', 'something', 'this', 'seems', 'not', 'see', 'the', 'his', 'again', \"'d\", 'doing', 'however', 'latter', 'am', 'wherever', 'perhaps', 'next', 'became', 'of', 'he', 'nothing', 'off', 'hereby', 'wherein', 'toward', 'nor', 'some', 'who', 'become', 'and', 'get', 'until', 'â€™ll', 'might', 'thus', 'often', 'such', 'after', 'bottom', 'someone', 'between', 'either', 'done', 'twelve', 'should', 'if', 'serious', 'be', 'say', 'three', 'still', 'regarding', 'we', 'keep', 'which', 'else', 'amongst', 'among', 'front', 'less', 'when', \"'re\", 'eleven', 'four', 'so', 'beside', 'thence', 'onto', 'anywhere', 'nobody', 'what', 'they', 'last', 'therefore', 'below', 'your', 'both', 'herein', 'own', 'every', 'unless', 'go', 'also', 'more', 'ten', 'neither', 'for', 'â€˜d', 'thereupon', 'former', 'twenty', 'almost', 'down', 'before', 'due', 'call', 'can', 'mostly', 'really', 'although', 'did', 'hers', 'becoming', 'their', 'whereas', 'where', 'would', 'that', 'than', 'out', 'from', 'me', 'has', 'there', 'never', 'at', 'name', 'few', 'even', 'hence', 'whom', 'show', \"'ll\", 'ourselves', 'six', 'was', 'afterwards', 'â€˜m', 'amount', 'just', 'seeming', 'using', 'you', 'whence', 'further', 'five', 'make', 'other', 'no', 'once', 'an', 'please', 'whereupon', 'since', 'whereby', 'them', 'up', 'in', 'behind', 'whether', 'herself', 'except', 'first', 'with', 'part', 'themselves', 'take', 'whatever', 'rather', 'noone', 'or', 'upon', 'besides', 'myself', 'towards', 'against', 'alone', 'us', 'back', 'being', 'are', 'enough', 'were', 'meanwhile', 'been', 'one', 'seemed', 'fifty', 'then', 'forty', 'together', 'â€™ve', 'everywhere', 'into', 'ever', 'ours', 'various', 'anyone', 'otherwise', 'sometime', 'without', 'therein', 'those', 'around', 'most', 'over', 'least', 'hundred', 'does', 'third', 'will', \"'ve\", 'hereupon', 'itself', 'fifteen', 'â€™re', 'indeed', 'why', 'everything', 'per', 'its', 'becomes', 'under', 'nowhere', 'well', 'himself', 'nâ€˜t', 'many', 'whereafter', 'already', 'empty', 'anything', 'had', 'on', 'yours', 'because', 'full', 'â€˜ve', 'beyond', \"n't\", 'another', 'any', 'by', 'beforehand', 'anyway', 'â€™d', 'somewhere', 're', 'somehow', 'could', 'thereby', 'anyhow', 'latterly', 'whither', 'much', 'along', \"'s\", 'thereafter', 'ca', 'nine', 'thru', 'two', 'â€˜s', 'whoever', 'nevertheless', 'â€™m', 'nâ€™t', 'elsewhere', 'sometimes', 'others', 'is', 'her', 'moreover', 'move', 'side', 'none', 'quite', 'several', 'too', 'mine', 'used', 'whole', 'always', 'whose'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72b3968d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[told, Dr., Lovato, tests, post, results, shortly, .]\n"
     ]
    }
   ],
   "source": [
    "print([token for token in doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70766092",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf512fa",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Spacy offers **Lemmatization** but not **Stemming**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c2aab58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'he'),\n",
       " ('told', 'tell'),\n",
       " ('Dr.', 'Dr.'),\n",
       " ('Lovato', 'Lovato'),\n",
       " ('that', 'that'),\n",
       " ('he', 'he'),\n",
       " ('was', 'be'),\n",
       " ('done', 'do'),\n",
       " ('with', 'with'),\n",
       " ('the', 'the'),\n",
       " ('tests', 'test'),\n",
       " ('and', 'and'),\n",
       " ('would', 'would'),\n",
       " ('post', 'post'),\n",
       " ('the', 'the'),\n",
       " ('results', 'result'),\n",
       " ('shortly', 'shortly'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.text, token.lemma_) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49add5e6",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "If you need stemming for your use case, you can use **NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df8b7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bbc58e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c88d03ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "singles = [porter_stemmer.stem(plural) for plural in plurals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4944c214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "724665e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "have\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "print(snowball_stemmer.stem(\"running\"))\n",
    "\n",
    "print(snowball_stemmer.stem(\"having\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e937bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous\n"
     ]
    }
   ],
   "source": [
    "print(snowball_stemmer.stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c355c5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gener\n"
     ]
    }
   ],
   "source": [
    "print(porter_stemmer.stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44cc6d",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a83de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Vijay watched Animal at the cinema and was ashamed of himself.\"\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1f59a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Vijay', 'PROPN'),\n",
       " ('watched', 'VERB'),\n",
       " ('Animal', 'PROPN'),\n",
       " ('at', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('cinema', 'NOUN'),\n",
       " ('and', 'CCONJ'),\n",
       " ('was', 'AUX'),\n",
       " ('ashamed', 'ADJ'),\n",
       " ('of', 'ADP'),\n",
       " ('himself', 'PRON'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.text, token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6710fac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tesla', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('massive', 'JJ'),\n",
       " ('$', '$'),\n",
       " ('30', 'CD'),\n",
       " ('bn', 'IN'),\n",
       " ('plan', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('India', 'NNP'),\n",
       " (';', ':'),\n",
       " ('Musk', 'NNP'),\n",
       " ('an', 'DT'),\n",
       " (\"'\", '``'),\n",
       " ('admirer', 'NN'),\n",
       " (\"'\", \"''\"),\n",
       " ('of', 'IN'),\n",
       " ('Modi', 'NNP')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.text, token.tag_) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b64e4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"NNP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f80dd",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97c1449b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tesla', 'ORG'),\n",
       " (\"'s\", ''),\n",
       " ('massive', ''),\n",
       " ('$', ''),\n",
       " ('30', 'MONEY'),\n",
       " ('bn', ''),\n",
       " ('plan', ''),\n",
       " ('for', ''),\n",
       " ('India', 'GPE'),\n",
       " (';', ''),\n",
       " ('Musk', 'PERSON'),\n",
       " ('an', ''),\n",
       " (\"'\", ''),\n",
       " ('admirer', ''),\n",
       " (\"'\", ''),\n",
       " ('of', ''),\n",
       " ('Modi', 'GPE')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Tesla's massive $30 bn plan for India; Musk an 'admirer' of Modi\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "[(token.text, token.ent_type_) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe9a327c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tesla', 'ORG'),\n",
       " ('30', 'MONEY'),\n",
       " ('India', 'GPE'),\n",
       " ('Musk', 'PERSON'),\n",
       " ('Modi', 'GPE')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.text, token.ent_type_) for token in doc if token.ent_type != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "690d7875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tesla', 'ORG'),\n",
       " ('30', 'MONEY'),\n",
       " ('India', 'GPE'),\n",
       " ('Musk', 'PERSON'),\n",
       " ('Modi', 'GPE')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baa603a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tesla', 'ORG', 0, 5),\n",
       " ('30', 'MONEY', 17, 19),\n",
       " ('India', 'GPE', 32, 37),\n",
       " ('Musk', 'PERSON', 39, 43),\n",
       " ('Modi', 'GPE', 60, 64)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d3a114d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "'s massive $\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    30\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " bn plan for \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    India\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "; \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Musk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " an 'admirer' of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Modi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27b09e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30dd11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
